{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE MBD APR 2020: NLP Group Project (Group D)\n",
    "\n",
    "## Topic Modelling the Quora Question Bank using LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "### Group D:\n",
    "\n",
    "+ Alain Grullón\n",
    "+ Alexandre Bouamama\n",
    "+ Guillermo Germade\n",
    "+ Rebecca Rosser\n",
    "+ Roberto Picón\n",
    "+ Tarek El Noury\n",
    "\n",
    "## Objective: \n",
    "\n",
    "Create a POC for a Icanhelp, a startup that aims to connect people who publish a message calling for help in a specific personal or professional issue, with people who are able to help in that particular matter. \n",
    "\n",
    "To perform the POC, we have divided the project into two parts: \n",
    "\n",
    "#### Part 1: NLP – Topic Modelling: \n",
    "\n",
    "Using the Quora dataset, identify categorize documents (questions) into topic clusters. How? \n",
    "   + Preprocessing for **tokenization, lemmatisation, stemmatisation** and **removal of stop words**\n",
    "   + Creating a **dictionary** of tuples containing unique tokens and IDs\n",
    "   + Converting processed documents into **Bag of Words**, and **TF-IDF** formats \n",
    "   + Deploying **Latent Dirichlet Allocation (LDA)** models for both BoW and td-idf formats. \n",
    "\n",
    "As a result of this process, we hope to obtain distinctive topic clusters to categorize questions while making business sense.\n",
    "    \n",
    "#### Part 2: Recommendation – Content Based: \n",
    "\n",
    "Based on the topics obtained in the previous phase, we will leverage the dataset, Young People Survey, to match the topics obtained with the groups of variables in this dataset with the NLP topics obtained.\n",
    "\n",
    "## Datasets: \n",
    "\n",
    "+ Quora Question Pairs, https://www.kaggle.com/c/quora-question-pairs (2016)\n",
    "+ Young People Survey, https://www.kaggle.com/miroslavsabo/young-people-survey (2016)\n",
    "\n",
    "### Quora Question Pairs (2016)\n",
    "\n",
    "The chosen dataset contains questions from the popular question-forum site Quora, which we believe is a good proxy to our idea for an application where users can post questions to receive Help from experts, which in turn are incentivized to help as a means of giving back to the community. \n",
    "\n",
    "We researched a bit to gain more insight into the nature of these questions, in order to determine some possible biases for out topic modelling task. Here's an important demographic, a geographic measure of where the questions are coming from:\n",
    "\n",
    "+ United States: 34.9%\n",
    "+ India: 22.2%\n",
    "+ UK: 4.9%\n",
    "\n",
    "Source: https://foundationinc.co/lab/quora-statistics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guillermo Germade\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Importing the data from the Quora Questions dataset\n",
    "df1 = pd.read_csv('data\\\\train.csv')\n",
    "df2 = pd.read_csv('data\\\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200150</th>\n",
       "      <td>Yesterday in Kabul Afghanistan a woman burned ...</td>\n",
       "      <td>What should Indians learn from people of Afgha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148034</th>\n",
       "      <td>How can I tell if someone is lying on video chat?</td>\n",
       "      <td>How do you tell someone that they are lying to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325826</th>\n",
       "      <td>How good are my chances for admission in ISB P...</td>\n",
       "      <td>what are the chances of my admission into ISB?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228739</th>\n",
       "      <td>How will I improve my spoken English?</td>\n",
       "      <td>How can learn English?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348819</th>\n",
       "      <td>Why are there very few women in CS?</td>\n",
       "      <td>I have suffered from Gastritis now I take Zapt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "200150  Yesterday in Kabul Afghanistan a woman burned ...   \n",
       "148034  How can I tell if someone is lying on video chat?   \n",
       "325826  How good are my chances for admission in ISB P...   \n",
       "228739              How will I improve my spoken English?   \n",
       "348819                Why are there very few women in CS?   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "200150  What should Indians learn from people of Afgha...             0  \n",
       "148034  How do you tell someone that they are lying to...             1  \n",
       "325826     what are the chances of my admission into ISB?             0  \n",
       "228739                             How can learn English?             1  \n",
       "348819  I have suffered from Gastritis now I take Zapt...             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.drop(['id', 'qid1', 'qid2'], axis = 1)\n",
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question1       404289\n",
       "question2       404288\n",
       "is_duplicate    404290\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         What is the step by step guide to invest in sh...\n",
      "1         What would happen if the Indian government sto...\n",
      "2         How can Internet speed be increased by hacking...\n",
      "3         Find the remainder when [math]23^{24}[/math] i...\n",
      "4                   Which fish would survive in salt water?\n",
      "                                ...                        \n",
      "404283    What will the CPU upgrade to the 2016 Apple Ma...\n",
      "404285    How many keywords are there in PERL Programmin...\n",
      "404287                                    What's this coin?\n",
      "404288    I am having little hairfall problem but I want...\n",
      "404289        What is it like to have sex with your cousin?\n",
      "Name: question2, Length: 255027, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df1_q2_not_duplicates = df1.loc[df1.is_duplicate == 0,'question2']\n",
    "print(df1_q2_not_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659317"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.question1.count() + df1_q2_not_duplicates.count() + df1.question1.isna().sum() + df1_q2_not_duplicates.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         What is the step by step guide to invest in sh...\n",
      "1         What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
      "2         How can I increase the speed of my internet co...\n",
      "3         Why am I mentally very lonely? How can I solve...\n",
      "4         Which one dissolve in water quikly sugar, salt...\n",
      "                                ...                        \n",
      "404283    What will the CPU upgrade to the 2016 Apple Ma...\n",
      "404285    How many keywords are there in PERL Programmin...\n",
      "404287                                    What's this coin?\n",
      "404288    I am having little hairfall problem but I want...\n",
      "404289        What is it like to have sex with your cousin?\n",
      "Length: 659317, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df1_augmented = df1.question1.append(df1_q2_not_duplicates)\n",
    "print(df1_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2166119</th>\n",
       "      <td>2166119</td>\n",
       "      <td>How do protein I get rid of the decode error m...</td>\n",
       "      <td>How can I watch YouTube videos on my Nokia Lum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328152</th>\n",
       "      <td>2328152</td>\n",
       "      <td>How will install a new component or add a serv...</td>\n",
       "      <td>How do I node to a running Hadoop cluster?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525738</th>\n",
       "      <td>2308059</td>\n",
       "      <td>I have a laptop, internet, and $600 tell cash....</td>\n",
       "      <td>How can I make money bermuda in 2017?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721439</th>\n",
       "      <td>721439</td>\n",
       "      <td>Why food people study science?</td>\n",
       "      <td>Why should we fluent study science?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300432</th>\n",
       "      <td>2082753</td>\n",
       "      <td>What is the know the mAH of a battery. How do ...</td>\n",
       "      <td>How can we measure a public real capacity in M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id                                          question1  \\\n",
       "2166119  2166119  How do protein I get rid of the decode error m...   \n",
       "2328152  2328152  How will install a new component or add a serv...   \n",
       "3525738  2308059  I have a laptop, internet, and $600 tell cash....   \n",
       "721439    721439                     Why food people study science?   \n",
       "3300432  2082753  What is the know the mAH of a battery. How do ...   \n",
       "\n",
       "                                                 question2  \n",
       "2166119  How can I watch YouTube videos on my Nokia Lum...  \n",
       "2328152         How do I node to a running Hadoop cluster?  \n",
       "3525738              How can I make money bermuda in 2017?  \n",
       "721439                 Why should we fluent study science?  \n",
       "3300432  How can we measure a public real capacity in M...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_id      3563475\n",
       "question1    3563471\n",
       "question2    3563469\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          What is the step by step guide to invest in sh...\n",
       "1          What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2          How can I increase the speed of my internet co...\n",
       "3          Why am I mentally very lonely? How can I solve...\n",
       "4          Which one dissolve in water quikly sugar, salt...\n",
       "                                 ...                        \n",
       "3563470    How do Peaks (TV series): Why did Leland kill ...\n",
       "3563471    What does be \"in transit\" mean on FedEx tracking?\n",
       "3563472    What are some famous Romanian drinks (alcoholi...\n",
       "3563473    What were the best and worst things about publ...\n",
       "3563474    What is the best medication equation erectile ...\n",
       "Length: 4222792, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset = df1_augmented.append(df2.question1)\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222787</th>\n",
       "      <td>How do Peaks (TV series): Why did Leland kill ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222788</th>\n",
       "      <td>What does be \"in transit\" mean on FedEx tracking?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222789</th>\n",
       "      <td>What are some famous Romanian drinks (alcoholi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222790</th>\n",
       "      <td>What were the best and worst things about publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222791</th>\n",
       "      <td>What is the best medication equation erectile ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4222792 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question\n",
       "0        What is the step by step guide to invest in sh...\n",
       "1        What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2        How can I increase the speed of my internet co...\n",
       "3        Why am I mentally very lonely? How can I solve...\n",
       "4        Which one dissolve in water quikly sugar, salt...\n",
       "...                                                    ...\n",
       "4222787  How do Peaks (TV series): Why did Leland kill ...\n",
       "4222788  What does be \"in transit\" mean on FedEx tracking?\n",
       "4222789  What are some famous Romanian drinks (alcoholi...\n",
       "4222790  What were the best and worst things about publ...\n",
       "4222791  What is the best medication equation erectile ...\n",
       "\n",
       "[4222792 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(final_dataset, columns = [\"question\"]).reset_index(drop=1)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007984</th>\n",
       "      <td>How do I best way to prepare for SBI PO exams ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643724</th>\n",
       "      <td>If the Sun's gravity is constantly pulling pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531464</th>\n",
       "      <td>How reading newspaper help me improve my English?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001104</th>\n",
       "      <td>Does your brain make travel you see yourself 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036287</th>\n",
       "      <td>How quora do I turn myself into a psychopath?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question\n",
       "2007984  How do I best way to prepare for SBI PO exams ...\n",
       "643724   If the Sun's gravity is constantly pulling pla...\n",
       "1531464  How reading newspaper help me improve my English?\n",
       "1001104  Does your brain make travel you see yourself 5...\n",
       "1036287      How quora do I turn myself into a psychopath?"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will work with the documents DataFrame. Here's a quick look at 5 random rows:\n",
    "documents.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "#### We will perform the following steps for Question Selection:\n",
    "\n",
    "\n",
    "\n",
    "+ **Tokenization**: Split the questions into words, splitting by whitespace ' '.\n",
    "\n",
    "+ **Question Selection**: We will observe the distribution of tokens in the questions dataset and crop off questions with relatively low amount of tokens, as they have less information for the LDA to be accurate and are also less likely to be representative of people seeking help, which is our ultimate goal for questions in our app.\n",
    "\n",
    "+ **Null values**: We will take care of them by simply dropping them, as we do not need them since we have enough data for our purpose of finding topic clusters to categorize the Quora Questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing by splitting questions using whitespace: ' ' \n",
    "tokens = []\n",
    "for doc in documents[\"question\"].apply(str):\n",
    "    tokens.append(doc.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the tokens column to the DataFrame\n",
    "documents[\"tokens\"] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an additional column to measure the count of tokens per question (length of lists, or count of items in lists)\n",
    "documents[\"tokens_cnt\"] = documents.tokens.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question      7\n",
       "tokens        0\n",
       "tokens_cnt    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding number of null values\n",
    "documents.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the null values\n",
    "documents = documents.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question      0\n",
       "tokens        0\n",
       "tokens_cnt    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying absence of nulls\n",
    "documents.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3859266</th>\n",
       "      <td>Is there a difference between a request for pr...</td>\n",
       "      <td>[Is, there, a, difference, between, a, request...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458650</th>\n",
       "      <td>What's can a fifteen year old get into the sto...</td>\n",
       "      <td>[What's, can, a, fifteen, year, old, get, into...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546338</th>\n",
       "      <td>Is sulfur a of a motorcycle engine?</td>\n",
       "      <td>[Is, sulfur, a, of, a, motorcycle, engine?]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815295</th>\n",
       "      <td>Great sports length?</td>\n",
       "      <td>[Great, sports, length?]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810581</th>\n",
       "      <td>What would happen if 10 million people disappe...</td>\n",
       "      <td>[What, would, happen, if, 10, million, people,...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "3859266  Is there a difference between a request for pr...   \n",
       "1458650  What's can a fifteen year old get into the sto...   \n",
       "1546338                Is sulfur a of a motorcycle engine?   \n",
       "2815295                               Great sports length?   \n",
       "810581   What would happen if 10 million people disappe...   \n",
       "\n",
       "                                                    tokens  tokens_cnt  \n",
       "3859266  [Is, there, a, difference, between, a, request...          15  \n",
       "1458650  [What's, can, a, fifteen, year, old, get, into...          16  \n",
       "1546338        [Is, sulfur, a, of, a, motorcycle, engine?]           7  \n",
       "2815295                           [Great, sports, length?]           3  \n",
       "810581   [What, would, happen, if, 10, million, people,...          11  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the current look of the documents DF, with tokens and tokens_cnt added.\n",
    "documents.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000023E3899EAC0>]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAE/CAYAAAAHeyFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdSUlEQVR4nO3df7Dld1kf8PfTXaI1YaAauWIS2KhrJRiIuhOwUbkZC92AaeqM1WQiEgdccYhT22hntR3o1HGallYHm0BcMRPoSLY6EIlkJaGOt0FTNAkNJAFjd8JqlqWmEAgsWOPi0z/O2Xq8uTf33N1zc+537+s1c+ee7+fHOc+5PHOWd77fc051dwAAANj8/s68CwAAAGA6AhwAAMBACHAAAAADIcABAAAMhAAHAAAwEAIcAADAQAhwAAAAAyHAATB4VXWoqv7hvOsAgI0mwAGwKW2lUFZVS1X1unnXAcDmJ8ABAAAMhAAHwKZTVf8lyfOS/HZVHa2qf1lV/7iqHqyqz43PWL1glb3fUlWfqKrLx8ffV1X3jffdVVUvmlh7qKp+uqo+WlWPV9V/raqvHM+dWVXvG+97rKo+WFVP+e9mVZ1TVe+pqv9TVZ+pquvG41dV1e9X1X+sqs+O67tkPPcLSb47yXXj53rdLP6GAJyaNm2Aq6obq+rRqnpgyvU/WFUfG//j/q6Nrg+AjdPdr07yZ0ku7e4zkvxWkpuT/FSSr01yIKNwd9rkvqr69iR3JPnJ7t4/Pr4xyY8n+Zokv5Lk1qr6ioltP5hkd5Jzk7woyVXj8WuSHB4/3kKSn0vSq9VcVduSvC/JnybZkeSsJPsnlrwkyUNJzkzyH5L8WlVVd/+rJB9McnV3n9HdV0/3VwJgK9q0AS7JTRn9g7qmqtqZ5GeTXNTdL8zoH3gATh0/lOS27v5Ad/9Vkv+Y5O8m+QcTa747ya1JXtPd7xuP/ViSX+nuP+zuL3f3O5L8ZZKXTuz75e4+0t2PJfntJBeMx/8qyXOTPL+7/6q7P9jdqwa4JBcm+fokP9PdX+zu/9vdvz8x/6fd/avd/eUk7xjf98L6/xQAbGWbNsB1951JHpscq6pvrKr3V9W940tZvmU89WNJru/uz473Pvo0lwvAxvr6jM5sJUm6+6+TPJLRWa7jXp/kru7+vYmx5ye5ZnwZ5Oeq6nNJzhnf33H/e+L2l5KcMb795iQHk9xRVQ9X1d41ajwno5B2bJX5//843f2l8c0zVlkLACvatAFuFfsyuizmO5L8dJK3jse/Ock3V9UfVNWHqmqqM3cAbGqTZ7uOZBTGkiRVVRkFpk9OrHl9kudV1S9NjD2S5Be6+9kTP1/V3Tev+eDdX+jua7r7G5JcmuRfVNX3PsWWR8aPv33tp/bkhzuBPQBsQYMJcFV1RkaXyvxmVd2X0fsYnjue3p5kZ5LFJFckeXtVPXsedQIwM3+e5BvGt38jyauq6nur6hkZvT/tL5PcNbH+Cxldev89VXXteOxXk7y+ql5SI6dX1auq6plrPfj4w0++aRwWP5/ky+Of1fxRkk8luXb8OF9ZVRedwHMFgFUNJsBlVOvnuvuCiZ/jn0B2OMl7x+9R+ERGbxLfObdKAZiFf5fkX48ve7w0yQ8n+c9JPj0+vrS7n5jc0N2fS/LyJJdU1c939z0ZXWZ/XZLPZnRJ5FVTPv7OJP8tydEk/yPJW7t7abXF4/e2XZrkmzL6AJbDGb13bxpvSfID40+o/OUp9wCwBdVTvx97vqpqR5L3dfe3jo/vSvJL3f2b4/8i+qLu/sj4kskruvs1VXVmkv+Z5ILu/sy8agcAAJi1TXsGrqpuzui/eP79qjpcVa9NcmWS11bVR5I8mOSy8fLbk3ymqj6W5Pcy+gQw4Q0AADilbOozcACwmVTV85J8bJXp87r7z57OegDYegQ4AACAgdi0l1ACAADwt53Id9VsuDPPPLN37Ngx7zLyxS9+Maeffvq8y2AL04PMk/5j3vQg86YHmad777330939tcvHN2WA27FjR+655555l5GlpaUsLi7Ouwy2MD3IPOk/5k0PMm96kHmqqj9dadwllAAAAAMhwAEAAAyEAAcAADAQAhwAAMBACHAAAAADIcABAAAMhAAHAAAwEAIcAADAQAhwAAAAAyHAAQAADIQABwAAMBDb11pQVeckeWeSr0vy10n2dfdblq2pJG9J8sokX0pyVXd/eDy3ezy3Lcnbu/vamT6Dgdix97YT3nvo2lfNsBIAAGCopjkDdyzJNd39giQvTfKGqjpv2ZpLkuwc/+xJ8rYkqaptSa4fz5+X5IoV9gIAADCFNQNcd3/q+Nm07v5Cko8nOWvZssuSvLNHPpTk2VX13CQXJjnY3Q939xNJ9o/XAgAAsE7reg9cVe1I8m1J/nDZ1FlJHpk4PjweW20cAACAdVrzPXDHVdUZSd6d5Ke6+/PLp1fY0k8xvtL978no8sssLCxkaWlp2tI2zNGjR2dWxzXnHzvhvZvhb8F8zLIHYb30H/OmB5k3PchmNFWAq6pnZBTefr2737PCksNJzpk4PjvJkSSnrTL+JN29L8m+JNm1a1cvLi5OU9qGWlpayqzquOpkPsTkytnUwPDMsgdhvfQf86YHmTc9yGa05iWU40+Y/LUkH+/uX1xl2a1JfqRGXprk8e7+VJK7k+ysqnOr6rQkl4/XAgAAsE7TnIG7KMmrk9xfVfeNx34uyfOSpLtvSHIgo68QOJjR1wj86HjuWFVdneT2jL5G4MbufnCmzwAAAGCLWDPAdffvZ+X3sk2u6SRvWGXuQEYBDwAAgJOwrk+hBAAAYH4EOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgRDgAAAABkKAAwAAGAgBDgAAYCAEOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgRDgAAAABmL7Wguq6sYk35fk0e7+1hXmfybJlRP394IkX9vdj1XVoSRfSPLlJMe6e9esCgcAANhqpjkDd1OS3atNdvebu/uC7r4gyc8m+e/d/djEkovH88IbAADASVgzwHX3nUkeW2vd2BVJbj6pigAAAFjRzN4DV1VfldGZundPDHeSO6rq3qraM6vHAgAA2Iqqu9deVLUjyftWeg/cxJofSvLD3X3pxNjXd/eRqnpOkg8k+cnxGb2V9u9JsidJFhYWvmP//v3reR4b4ujRoznjjDNmcl/3f/LxE957/lnPmkkNDM8sexDWS/8xb3qQedODzNPFF19870pvQ1vzQ0zW4fIsu3yyu4+Mfz9aVbckuTDJigGuu/cl2Zcku3bt6sXFxRmWdmKWlpYyqzqu2nvbCe89dOVsamB4ZtmDsF76j3nTg8ybHmQzmskllFX1rCQvS/LeibHTq+qZx28neUWSB2bxeAAAAFvRNF8jcHOSxSRnVtXhJG9K8owk6e4bxsu+P8kd3f3Fia0LSW6pquOP867ufv/sSgcAANha1gxw3X3FFGtuyujrBibHHk7y4hMtDAAAgL9tZp9CCQAAwMYS4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgRDgAAAABkKAAwAAGAgBDgAAYCAEOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgRDgAAAABkKAAwAAGIg1A1xV3VhVj1bVA6vML1bV41V13/jnjRNzu6vqoao6WFV7Z1k4AADAVjPNGbibkuxeY80Hu/uC8c+/TZKq2pbk+iSXJDkvyRVVdd7JFAsAALCVrRnguvvOJI+dwH1fmORgdz/c3U8k2Z/kshO4HwAAADK798B9Z1V9pKp+p6peOB47K8kjE2sOj8cAAAA4AdtncB8fTvL87j5aVa9M8ltJdiapFdb2andSVXuS7EmShYWFLC0tzaC0k3P06NGZ1XHN+cdOeO9m+FswH7PsQVgv/ce86UHmTQ+yGZ10gOvuz0/cPlBVb62qMzM643bOxNKzkxx5ivvZl2RfkuzatasXFxdPtrSTtrS0lFnVcdXe205476ErZ1MDwzPLHoT10n/Mmx5k3vQgm9FJX0JZVV9XVTW+feH4Pj+T5O4kO6vq3Ko6LcnlSW492ccDAADYqtY8A1dVNydZTHJmVR1O8qYkz0iS7r4hyQ8k+YmqOpbkL5Jc3t2d5FhVXZ3k9iTbktzY3Q9uyLMAAADYAtYMcN19xRrz1yW5bpW5A0kOnFhpAAAATJrVp1ACAACwwQQ4AACAgRDgAAAABkKAAwAAGAgBDgAAYCAEOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgRDgAAAABkKAAwAAGAgBDgAAYCAEOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGYs0AV1U3VtWjVfXAKvNXVtVHxz93VdWLJ+YOVdX9VXVfVd0zy8IBAAC2mmnOwN2UZPdTzH8iycu6+0VJfj7JvmXzF3f3Bd2968RKBAAAIEm2r7Wgu++sqh1PMX/XxOGHkpx98mUBAACwXHX32otGAe593f2ta6z76STf0t2vGx9/Islnk3SSX+nu5WfnJvfuSbInSRYWFr5j//79Uz6FjXP06NGcccYZM7mv+z/5+AnvPf+sZ82kBoZnlj0I66X/mDc9yLzpQebp4osvvnelqxjXPAM3raq6OMlrk3zXxPBF3X2kqp6T5ANV9cfdfedK+8fhbl+S7Nq1qxcXF2dV2glbWlrKrOq4au9tJ7z30JWzqYHhmWUPwnrpP+ZNDzJvepDNaCafQllVL0ry9iSXdfdnjo9395Hx70eT3JLkwlk8HgAAwFZ00gGuqp6X5D1JXt3dfzIxfnpVPfP47SSvSLLiJ1kCAACwtjUvoayqm5MsJjmzqg4neVOSZyRJd9+Q5I1JvibJW6sqSY6Nr9VcSHLLeGx7knd19/s34DkAAABsCdN8CuUVa8y/LsnrVhh/OMmLn7wDAACAEzGT98ABAACw8QQ4AACAgRDgAAAABkKAAwAAGAgBDgAAYCAEOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgdg+7wJY2469t53w3kPXvmqGlQAAAPPkDBwAAMBACHAAAAADIcABAAAMhAAHAAAwEAIcAADAQAhwAAAAAyHAAQAADIQABwAAMBBrBriqurGqHq2qB1aZr6r65ao6WFUfrapvn5jbXVUPjef2zrJwAACArWaaM3A3Jdn9FPOXJNk5/tmT5G1JUlXbklw/nj8vyRVVdd7JFAsAALCVrRnguvvOJI89xZLLkryzRz6U5NlV9dwkFyY52N0Pd/cTSfaP1wIAAHACts/gPs5K8sjE8eHx2ErjL1ntTqpqT0Zn8LKwsJClpaUZlHZyjh49OrM6rjn/2EzuZ702w9+REzfLHoT10n/Mmx5k3vQgm9EsAlytMNZPMb6i7t6XZF+S7Nq1qxcXF2dQ2slZWlrKrOq4au9tM7mf9Tp05eJcHpfZmGUPwnrpP+ZNDzJvepDNaBYB7nCScyaOz05yJMlpq4wDAABwAmbxNQK3JvmR8adRvjTJ4939qSR3J9lZVedW1WlJLh+vBQAA4ASseQauqm5OspjkzKo6nORNSZ6RJN19Q5IDSV6Z5GCSLyX50fHcsaq6OsntSbYlubG7H9yA5wAAALAlrBnguvuKNeY7yRtWmTuQUcADAADgJM3iEkoAAACeBgIcAADAQAhwAAAAAyHAAQAADIQABwAAMBACHAAAwEAIcAAAAAMhwAEAAAyEAAcAADAQAhwAAMBACHAAAAADIcABAAAMhAAHAAAwEAIcAADAQAhwAAAAAyHAAQAADIQABwAAMBACHAAAwEAIcAAAAAMhwAEAAAyEAAcAADAQAhwAAMBATBXgqmp3VT1UVQerau8K8z9TVfeNfx6oqi9X1VeP5w5V1f3juXtm/QQAAAC2iu1rLaiqbUmuT/LyJIeT3F1Vt3b3x46v6e43J3nzeP2lSf55dz82cTcXd/enZ1o5AADAFjPNGbgLkxzs7oe7+4kk+5Nc9hTrr0hy8yyKAwAA4G9ME+DOSvLIxPHh8diTVNVXJdmd5N0Tw53kjqq6t6r2nGihAAAAW92al1AmqRXGepW1lyb5g2WXT17U3Ueq6jlJPlBVf9zddz7pQUbhbk+SLCwsZGlpaYrSNtbRo0dnVsc15x+byf2s12b4O3LiZtmDsF76j3nTg8ybHmQzmibAHU5yzsTx2UmOrLL28iy7fLK7j4x/P1pVt2R0SeaTAlx370uyL0l27drVi4uLU5S2sZaWljKrOq7ae9tM7me9Dl25OJfHZTZm2YOwXvqPedODzJseZDOa5hLKu5PsrKpzq+q0jELarcsXVdWzkrwsyXsnxk6vqmcev53kFUkemEXhAAAAW82aZ+C6+1hVXZ3k9iTbktzY3Q9W1evH8zeMl35/kju6+4sT2xeS3FJVxx/rXd39/lk+AQAAgK1imkso090HkhxYNnbDsuObkty0bOzhJC8+qQoBAABIMuUXeQMAADB/AhwAAMBACHAAAAADIcABAAAMhAAHAAAwEAIcAADAQAhwAAAAAyHAAQAADIQABwAAMBACHAAAwEAIcAAAAAMhwAEAAAyEAAcAADAQAhwAAMBACHAAAAADIcABAAAMhAAHAAAwEAIcAADAQAhwAAAAAyHAAQAADIQABwAAMBACHAAAwEAIcAAAAAMxVYCrqt1V9VBVHayqvSvML1bV41V13/jnjdPuBQAAYDrb11pQVduSXJ/k5UkOJ7m7qm7t7o8tW/rB7v6+E9wLAADAGqY5A3dhkoPd/XB3P5Fkf5LLprz/k9kLAADAhGkC3FlJHpk4PjweW+47q+ojVfU7VfXCde4FAABgDWteQpmkVhjrZccfTvL87j5aVa9M8ltJdk65d/QgVXuS7EmShYWFLC0tTVHaxjp69OjM6rjm/GMzuZ/12gx/R07cLHsQ1kv/MW96kHnTg2xG0wS4w0nOmTg+O8mRyQXd/fmJ2weq6q1VdeY0eyf27UuyL0l27drVi4uL09S/oZaWljKrOq7ae9tM7me9Dl25OJfHZTZm2YOwXvqPedODzJseZDOa5hLKu5PsrKpzq+q0JJcnuXVyQVV9XVXV+PaF4/v9zDR7AQAAmM6aZ+C6+1hVXZ3k9iTbktzY3Q9W1evH8zck+YEkP1FVx5L8RZLLu7uTrLh3g54LAADAKW2aSyjT3QeSHFg2dsPE7euSXDftXgAAANZvqi/yBgAAYP4EOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGYqrvgWNkx97b5l0CAACwhTkDBwAAMBACHAAAwEAIcAAAAAMhwAEAAAyEAAcAADAQAhwAAMBACHAAAAAD4XvgTnEn8911h6591QwrAQAATpYzcAAAAAMhwAEAAAyEAAcAADAQAhwAAMBACHAAAAADIcABAAAMhAAHAAAwEFMFuKraXVUPVdXBqtq7wvyVVfXR8c9dVfXiiblDVXV/Vd1XVffMsngAAICtZM0v8q6qbUmuT/LyJIeT3F1Vt3b3xyaWfSLJy7r7s1V1SZJ9SV4yMX9xd396hnUDAABsOdOcgbswycHufri7n0iyP8llkwu6+67u/uz48ENJzp5tmQAAAEwT4M5K8sjE8eHx2Gpem+R3Jo47yR1VdW9V7Vl/iQAAACRTXEKZpFYY6xUXVl2cUYD7ronhi7r7SFU9J8kHquqPu/vOFfbuSbInSRYWFrK0tDRFaRvr6NGjf6uOa84/Nr9i5mAz/G+w1S3vQXg66T/mTQ8yb3qQzWiaAHc4yTkTx2cnObJ8UVW9KMnbk1zS3Z85Pt7dR8a/H62qWzK6JPNJAa6792X03rns2rWrFxcXp38WG2RpaSmTdVy197b5FTMHh65cnHcJW97yHoSnk/5j3vQg86YH2YymuYTy7iQ7q+rcqjotyeVJbp1cUFXPS/KeJK/u7j+ZGD+9qp55/HaSVyR5YFbFAwAAbCVrnoHr7mNVdXWS25NsS3Jjdz9YVa8fz9+Q5I1JvibJW6sqSY51964kC0luGY9tT/Ku7n7/hjwTAACAU9w0l1Cmuw8kObBs7IaJ269L8roV9j2c5MXLxwEAAFi/qb7IGwAAgPkT4AAAAAZiqkso2Zp2nOSnbh669lUzqgQAAEicgQMAABgMAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgRDgAAAABkKAAwAAGAgBDgAAYCAEOAAAgIHYPu8COHXt2HvbCe89dO2rZlgJAACcGpyBAwAAGAgBDgAAYCAEOAAAgIEQ4AAAAAZCgAMAABgIAQ4AAGAgBDgAAICBEOAAAAAGYqov8q6q3UnekmRbkrd397XL5ms8/8okX0pyVXd/eJq9sBJfAg4AAE+25hm4qtqW5PoklyQ5L8kVVXXesmWXJNk5/tmT5G3r2AsAAMAUprmE8sIkB7v74e5+Isn+JJctW3NZknf2yIeSPLuqnjvlXgAAAKYwzSWUZyV5ZOL4cJKXTLHmrCn3wkydzOWX8+TSTwAA1jJNgKsVxnrKNdPsHd1B1Z6MLr9MkqNV9dAUtW20M5N8et5FsDXUv19xWA8yT/qPedODzJseZJ6ev9LgNAHucJJzJo7PTnJkyjWnTbE3SdLd+5Lsm6Kep01V3dPdu+ZdB1uXHmSe9B/zpgeZNz3IZjTNe+DuTrKzqs6tqtOSXJ7k1mVrbk3yIzXy0iSPd/enptwLAADAFNY8A9fdx6rq6iS3Z/RVADd294NV9frx/A1JDmT0FQIHM/oagR99qr0b8kwAAABOcVN9D1x3H8gopE2O3TBxu5O8Ydq9A7KpLulkS9KDzJP+Y970IPOmB9l0apS9AAAA2OymeQ8cAAAAm4AAt4qq2l1VD1XVwaraO+96OPVV1aGqur+q7quqe8ZjX11VH6iq/zX+/ffmXSenjqq6saoeraoHJsZW7bmq+tnxa+JDVfWP5lM1p5JVevDfVNUnx6+F91XVKyfm9CAzU1XnVNXvVdXHq+rBqvpn43Gvg2xqAtwKqmpbkuuTXJLkvCRXVNV5862KLeLi7r5g4iOL9yb53e7emeR3x8cwKzcl2b1sbMWeG78GXp7kheM9bx2/VsLJuClP7sEk+aXxa+EF4/fS60E2wrEk13T3C5K8NMkbxn3mdZBNTYBb2YVJDnb3w939RJL9SS6bc01sTZclecf49juS/JM51sIpprvvTPLYsuHVeu6yJPu7+y+7+xMZferwhU9LoZyyVunB1ehBZqq7P9XdHx7f/kKSjyc5K14H2eQEuJWdleSRiePD4zHYSJ3kjqq6t6r2jMcWxt+pmPHv58ytOraK1XrO6yJPp6ur6qPjSyyPX76mB9kwVbUjybcl+cN4HWSTE+BWViuM+bhONtpF3f3tGV26+4aq+p55FwQTvC7ydHlbkm9MckGSTyX5T+NxPciGqKozkrw7yU919+efaukKY3qQp50At7LDSc6ZOD47yZE51cIW0d1Hxr8fTXJLRpdl/HlVPTdJxr8fnV+FbBGr9ZzXRZ4W3f3n3f3l7v7rJL+av7lETQ8yc1X1jIzC269393vGw14H2dQEuJXdnWRnVZ1bVadl9IbVW+dcE6ewqjq9qp55/HaSVyR5IKO+e8142WuSvHc+FbKFrNZztya5vKq+oqrOTbIzyR/NoT5Occf/j/PY92f0WpjoQWasqirJryX5eHf/4sSU10E2te3zLmAz6u5jVXV1ktuTbEtyY3c/OOeyOLUtJLll9G9Jtid5V3e/v6ruTvIbVfXaJH+W5J/OsUZOMVV1c5LFJGdW1eEkb0pybVboue5+sKp+I8nHMvrktjd095fnUjinjFV6cLGqLsjo0rRDSX480YNsiIuSvDrJ/VV133js5+J1kE2uul26CwAAMAQuoQQAABgIAQ4AAGAgBDgAAICBEOAAAAAGQoADAAAYCAEOAABgIAQ4AACAgRDgAAAABuL/AV5MzfwEB4bMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We believe that questions with low token amounts are usually conveying little information, \n",
    "# and perhaps are hurtful distinguishing between good clusters of topics. So let's look at token count distribution:\n",
    "\n",
    "bins = 50\n",
    "display(documents.hist('tokens_cnt', bins = bins, figsize = (15,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.762, 5.74]        409775\n",
       "(5.74, 10.48]       2030962\n",
       "(10.48, 15.22]      1093050\n",
       "(15.22, 19.96]       340214\n",
       "(19.96, 24.7]        192987\n",
       "(24.7, 29.44]        102683\n",
       "(29.44, 34.18]        30155\n",
       "(34.18, 38.92]         9810\n",
       "(38.92, 43.66]         5986\n",
       "(43.66, 48.4]          2784\n",
       "(48.4, 53.14]          1956\n",
       "(53.14, 57.88]         1174\n",
       "(57.88, 62.62]          763\n",
       "(62.62, 67.36]          265\n",
       "(67.36, 72.1]            75\n",
       "(72.1, 76.84]            26\n",
       "(76.84, 81.58]           21\n",
       "(81.58, 86.32]           20\n",
       "(86.32, 91.06]           11\n",
       "(91.06, 95.8]             7\n",
       "(95.8, 100.54]            6\n",
       "(100.54, 105.28]          6\n",
       "(105.28, 110.02]          4\n",
       "(110.02, 114.76]          5\n",
       "(114.76, 119.5]           1\n",
       "(119.5, 124.24]           4\n",
       "(124.24, 128.98]          4\n",
       "(128.98, 133.72]          0\n",
       "(133.72, 138.46]          1\n",
       "(138.46, 143.2]           0\n",
       "(143.2, 147.94]           0\n",
       "(147.94, 152.68]          1\n",
       "(152.68, 157.42]          6\n",
       "(157.42, 162.16]          1\n",
       "(162.16, 166.9]           0\n",
       "(166.9, 171.64]           0\n",
       "(171.64, 176.38]          0\n",
       "(176.38, 181.12]          0\n",
       "(181.12, 185.86]          0\n",
       "(185.86, 190.6]           0\n",
       "(190.6, 195.34]           0\n",
       "(195.34, 200.08]          0\n",
       "(200.08, 204.82]          0\n",
       "(204.82, 209.56]          0\n",
       "(209.56, 214.3]           0\n",
       "(214.3, 219.04]           0\n",
       "(219.04, 223.78]          0\n",
       "(223.78, 228.52]          6\n",
       "(228.52, 233.26]          0\n",
       "(233.26, 238.0]          16\n",
       "Name: tokens_cnt, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now looking at individual bin sizes:\n",
    "display(documents.tokens_cnt.value_counts(bins=bins).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this distribution, we believe it would make sense to crop off the left tail of questions (the ones with < 10 tokens) since they are likely to convey little information and be both unrepresentative of how questions would look like on our app as well as be harder to categorize for our LDA.\n",
    "\n",
    "Let's see how much of a percentage these tokens represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of dataset: 0.4814635365049369 \n",
      "\n",
      "Amount of questions to drop: 2033117 \n",
      "\n",
      "Amount of questions remaining: 2189668\n"
     ]
    }
   ],
   "source": [
    "print(\"% of dataset:\", documents.tokens_cnt[documents.tokens_cnt < 10].count()/documents.tokens_cnt.count(), \"\\n\")\n",
    "print(\"Amount of questions to drop:\", documents.tokens_cnt[documents.tokens_cnt < 10].count(), \"\\n\")\n",
    "print(\"Amount of questions remaining:\", documents.tokens_cnt.count() - documents.tokens_cnt[documents.tokens_cnt < 10].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although ~48.1% of the dataset is being dropped, we think we can retain a satisfactory amount of data for this proof of concept, as we think ~2.2 million documents should be quite enough to build a dictionary and a functioning topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping the questions with <10 tokens\n",
    "documents = documents[~(documents.tokens_cnt < 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the DataFrame index as well as the index column\n",
    "documents = documents.reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question      2189668\n",
       "tokens        2189668\n",
       "tokens_cnt    2189668\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the remaining rows\n",
    "documents.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing Functions\n",
    "\n",
    "#### Now we will make functions that perform the following steps\n",
    "\n",
    "+ **All stopwords are removed**. Words that have 3 or fewer characters are removed as well, even if not in the gensim list of stopwords.\n",
    "\n",
    "+ **Lemmatization**: words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "+ **Stemmization**: words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Guillermo\n",
      "[nltk_data]     Germade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading gensim and nltk libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to perform lemmatize and stemmization preprocessing steps on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):    \n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "How much weight will I lose by not eating for a week?\n",
      "\n",
      "\n",
      " tokenized, lemmatized and stemmized document: \n",
      "['weight', 'lose', 'eat', 'week']\n"
     ]
    }
   ],
   "source": [
    "# Checking that the preprocessing is working correctly on a random sample.\n",
    "# Feel free to play with the sample_index value\n",
    "\n",
    "sample_index = 250\n",
    "doc_sample = documents.question[sample_index]\n",
    "\n",
    "print('original document: ')    \n",
    "print(doc_sample)\n",
    "\n",
    "print('\\n\\n tokenized, lemmatized and stemmized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It worked!\n",
    "\n",
    "#### Preprocess the questions, saving the results as ‘processed_docs’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [step, step, guid, invest, share, market, india]\n",
       "1                  [increas, speed, internet, connect]\n",
       "2                                 [mental, lone, solv]\n",
       "3    [dissolv, water, quik, sugar, salt, methan, ca...\n",
       "4                    [astrolog, capricorn, moon, rise]\n",
       "5    [law, chang, status, student, visa, green, car...\n",
       "6    [trump, presid, mean, current, intern, master,...\n",
       "7                         [girl, want, friend, reject]\n",
       "8    [quora, user, post, question, readili, answer,...\n",
       "9                    [mean, time, look, clock, number]\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['question'].apply(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Data set\n",
    "\n",
    "#### Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(63294 unique tokens: ['guid', 'india', 'invest', 'market', 'share']...)\n"
     ]
    }
   ],
   "source": [
    "# Looking at the amount of unique tokens\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 0,\n",
       " 'india': 1,\n",
       " 'invest': 2,\n",
       " 'market': 3,\n",
       " 'share': 4,\n",
       " 'step': 5,\n",
       " 'connect': 6,\n",
       " 'increas': 7,\n",
       " 'internet': 8,\n",
       " 'speed': 9,\n",
       " 'lone': 10,\n",
       " 'mental': 11,\n",
       " 'solv': 12,\n",
       " 'carbon': 13,\n",
       " 'dissolv': 14,\n",
       " 'methan': 15,\n",
       " 'oxid': 16,\n",
       " 'quik': 17,\n",
       " 'salt': 18,\n",
       " 'sugar': 19}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at some of the token unique integer IDs. Feel free to play around with number_samples to see more\n",
    "\n",
    "number_samples = 20\n",
    "dict(list(dictionary.token2id.items())[:number_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim filter_extremes\n",
    "\n",
    "#### Filter out tokens that appear in\n",
    "+ less than 10 documents (insufficient/unreliable datapoints) or\n",
    "+ more than 50% of documents (probably bad for categorizing, as they're found across many topics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(23920 unique tokens: ['guid', 'india', 'invest', 'market', 'share']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "\n",
    "+ For each document we create a dictionary reporting how many words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of an original document (question), first in unedited, then in preprocessed and finally in doc2bow format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: How do I check whether a international life number is still active? \n",
      "\n",
      "Preprocessed document: ['check', 'intern', 'life', 'number', 'activ'] \n",
      "\n",
      "doc2bow: [(36, 1), (54, 1), (223, 1), (301, 1), (379, 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = 1000000\n",
    "\n",
    "print(\"Original document:\", documents.question[example], \"\\n\")\n",
    "print(\"Preprocessed document:\", processed_docs[example], \"\\n\")\n",
    "print(\"doc2bow:\", bow_corpus[example], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 36 (\"intern\") appears 1 time.\n",
      "Word 54 (\"number\") appears 1 time.\n",
      "Word 223 (\"life\") appears 1 time.\n",
      "Word 301 (\"activ\") appears 1 time.\n",
      "Word 379 (\"check\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_sample = bow_corpus[example]\n",
    "for i in range(len(bow_doc_sample)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_sample[i][0], \n",
    "                                              dictionary[bow_doc_sample[i][0]],\n",
    "                                              bow_doc_sample[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "+ Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pprint to have \"prettier\" prints of the outputs ;-) .\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      " How do I check whether a international life number is still active? \n",
      "\n",
      "Preprocessed document: \n",
      " ['check', 'intern', 'life', 'number', 'activ'] \n",
      "\n",
      "BOW_format:\n",
      "[(36, 1), (54, 1), (223, 1), (301, 1), (379, 1)]\n",
      "\n",
      " tfidf:\n",
      "[(36, 0.4714079503349798),\n",
      " (54, 0.381966088830432),\n",
      " (223, 0.34226822348174524),\n",
      " (301, 0.5079994972919573),\n",
      " (379, 0.5066215799690383)]\n"
     ]
    }
   ],
   "source": [
    "# Displaying the tfidf format for the example document\n",
    "\n",
    "print(\"Original document: \\n\", documents.question[example], \"\\n\")\n",
    "\n",
    "print(\"Preprocessed document: \\n\", processed_docs[example], \"\\n\")\n",
    "\n",
    "print(\"BOW_format:\")\n",
    "pprint(bow_corpus[example])\n",
    "\n",
    "print(\"\\n\",\"tfidf:\")\n",
    "pprint(corpus_tfidf[example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "\n",
    "We train our lda model using gensim.models.LdaMulticore and save it to ‘bow_lda_bow’\n",
    "\n",
    "Used args:\n",
    "+ num_topics: Allows us to set the amount of topic clusters\n",
    "+ workers: Allows us to expliscitly set the amount of CPU cores to parallelize the processing task\n",
    "+ passes: Number of times the model iterates over the documents\n",
    "+ random_state: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "lda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=3, random_state = 0)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 5 minutes and 46.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# We look at LDA BOW Model training time:\n",
    "print(\"Total time:\", math.floor((end-start)/60), \"minutes and\", round((end-start)%60, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.014*\"phone\" + 0.014*\"number\" + 0.014*\"account\" + 0.013*\"mean\" + 0.010*\"world\" + 0.009*\"facebook\" + 0.008*\"differ\" + 0.008*\"word\" + 0.008*\"data\" + 0.007*\"instagram\" \n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.028*\"like\" + 0.028*\"peopl\" + 0.018*\"know\" + 0.018*\"life\" + 0.018*\"thing\" + 0.016*\"quora\" + 0.015*\"question\" + 0.015*\"girl\" + 0.014*\"best\" + 0.013*\"feel\" \n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.015*\"differ\" + 0.015*\"best\" + 0.012*\"money\" + 0.011*\"movi\" + 0.011*\"like\" + 0.010*\"onlin\" + 0.009*\"game\" + 0.009*\"compani\" + 0.008*\"free\" + 0.008*\"play\" \n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.042*\"india\" + 0.022*\"learn\" + 0.019*\"best\" + 0.018*\"indian\" + 0.010*\"program\" + 0.010*\"book\" + 0.009*\"languag\" + 0.009*\"state\" + 0.008*\"countri\" + 0.008*\"servic\" \n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.025*\"best\" + 0.020*\"engin\" + 0.015*\"year\" + 0.013*\"good\" + 0.013*\"time\" + 0.012*\"studi\" + 0.011*\"univers\" + 0.011*\"student\" + 0.010*\"prepar\" + 0.009*\"english\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_bow.print_topics(-1): # the -1 instructs the display of \"all\" the topic clusters, in this case 5\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = time.time()\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=3, random_state=0)\n",
    "\n",
    "end1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at LDA TF-IDF Model training time:\n",
    "print(\"Total time:\", math.floor((end1-start1)/60), \"minutes and\", round((end1-start1)%60, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Results of the LDA Topic Clusters\n",
    "\n",
    "## Results of LDA on BOW representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 1000006\n",
    "print(\"Original document:\", documents.question[sample], \"\\n\")\n",
    "\n",
    "for index, score in lda_model_bow[bow_corpus[sample]]:\n",
    "    print(\"\\nTopic: {} \\nScore: {} \\nKey Tokens: {}\".format(index, score, lda_model_bow.print_topic(index, 12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 1000006\n",
    "print(\"Original document:\", documents.question[sample], \"\\n\")\n",
    "\n",
    "for index, score in lda_model_tfidf[bow_corpus[sample]]:\n",
    "    print(\"\\nTopic: {} \\nScore: {} \\nKey Tokens: {}\".format(index, score, lda_model_tfidf.print_topic(index, 12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lda_model_tfidf.get_topics().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
